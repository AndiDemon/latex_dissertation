% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Time Series Self-Attention 2D Human Motion Forecasting}\label{3} % top level followed by section, subsection
% Time Series Self-Attention for 2D Human Motion Forecasting using Annotated Data

% ----------------------- contents from here ------------------------

\section{Introduction}
With the development of the autonomous system such as auto-driving cars and auto-pilot robots that can be utilized in many ways~\cite{autonomous_driving, autonomous_driving_road, autonomous_robot_nature}, the safety system prevents the device or vehicle from crashing unexpectedly into other objects like humans.
Such a system in real life is still under development, but some systems using sensors and cameras on devices are implemented on electric vehicles or non-electric vehicles. With more advanced technology, this system can be expanded to the level of knowing the behavior of the environment.
Moreover, several applications can be utilized by knowing human motion forecasting.
For example, human motion forecasting can help the human motion tracking model for better accuracy~\cite{motion_tracking}.
It can be used in the locomotive syndrome disorder evaluation to prevent humans from falling or any self-accident that might happen, as well as gait recognition, to identify patterns during walking~\cite{jaciii_gait}.
%It can be used in the locomotive syndrome disorder evaluation and used to prevent humans from falling down, etc.

Human motion forecasting has been retracted attention for advancing methods, strategies, and results. Several types of research, such as using recurrent neural networks, gated recurrent units (GRU), long short-term memory (LSTM), and Transformer Networks, are conducted in many ways for this problem~\cite{jain2016, martinez2017, chiu2019, Mao2019, wang2021}. However, the inputs and outputs they generate are based on data obtained from 3D motion capture~\cite{motionmixer2022, Sofianos2021}. This is not directly applicable to the real world when using the 2D input image generated by the pose estimation on an RGB camera. Furthermore, this work is challenging considering that human behaviors are dynamic based on the person. With this in mind, the scope of our research should be shrinker by uncomplicated motions provided in the dataset.

Inspired by the natural language processing (NLP) problem,
the attention scheme has been showing excellent results in understanding the context and connection between words in sentences~\cite{bert_understanding}.
This attention scheme seems to be applicable in the sequence-to-sequence time series data, which has a similar conception to the words in natural language processing. When in the NLP task, the word vector is taken as the input on each sentence, as for the time-series data, a single time point data is taken like the word in the NLP task.

In this research, the author proposes the core of behavioral human motion forecasting as a step to realizing and advancing behavior-based knowledge systems for autonomous systems. 
In order to result in the calculated future movement of the human, the input of image sequences with the human body pose feature is required.
The author constructed the human motion forecasting algorithm by using the attention-based method since it shows promising results in many ways of usage and application~\cite{visionTransformer, vaswani2017, neo2020, vitae, bert_understanding, bert-aggression, mobilevit}.

The experiment is conducted using the Human3.6M dataset as the primary dataset and the 3DPW dataset as the secondary dataset.
In \textbf{Fig.~\ref{fig:sitting400}}, samples of the testing sequence ``sitting'' and ``direction'' motions have been tested by the model to obtain the 2D visualization for comparison with the ground truth.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./3/figures/sitting_direction.pdf}
    \caption{Pose prediction sequences of ``Sitting'' and ``Direction'' motions with a time-series self-attention network. The blue line is the ground truth, while the green line is the prediction result by the model.}
    \label{fig:sitting400}
\end{figure}
The Human3.6M dataset provides the human body's key points in every frame used for the proposed model input.
In this case, the method does not need to determine the input length since the data has the same size in each frame.
The input data is already a numerical value. Unlike the word in the NLP task, the proposed method does not perform the embedding that converts the input data into a numerical feature vector.
Instead, the process of positional encoding becomes extensive for understanding the key point connections by frames.
Thus, this research proposes positional encoding based on the frame with the transformer encoder-based method to predict human motion.

\section{Related Works}
\subsection{Human Motion Forecasting}
\subsubsection{3D Human Motion Forecasting}
As the 3D human motion prediction baseline, the Encoder-Recurrent-Decoder (ERD) model was introduced in 2015~\cite{fragkiadaki2015recurrent}. This research uses the recurrent neural network (RNN) architecture that absorbs nonlinear encoder and decoder networks before and after recurrent layers as the model's input. The Human3.6M dataset~\cite{human3.6m} is used and expected to obtain the short-term human motion prediction with 80, 160, 240, 320, 400, 480, and 560 milliseconds. As a result, the three layers of LSTM obtained the best result compared to ERD, Conditional Restricted Boltzmann Machines (CRBMs) model, Gaussian Process Dynamic Model (GPDM) model, and the nearest neighbor N-gram model (NGRAM) based on the Euclidean distance loss. 

Approaches from different methods and strategies have been broadly applied to solve the problem of human motion prediction.
One approach uses the state-of-the-art recurrent neural network (RNN) model~\cite{martinez2017}, which compares several methods as evaluation from other techniques, including LSTM. The goal is to learn time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion prediction.
The seq2seq, the encoding and decoding architecture, is used with the sampling-based loss for the long-term motion prediction~\cite{sutskever2014}. The research also employs the residual network to help the model to incorporate prior knowledge about the statistics of human motion.

More methods were published for this problem as it became popular among autonomous system researchers and developers. Different ways to predict better and to compute reliable standard of error are developed with the following competing results~\cite{jain2016, wang2019, chiu2019, Mao2019}. The mean per joint position error (MPJPE) is a reliable evaluation metric to show the distance between the ground truth and the prediction. One of the most recent human motion forecasting approaches is the Space-Time-Separable Graph Convolutional Network (STS-GCN)~\cite{Sofianos2021}, which also evaluated on the archive of motion capture as surface shapes (AMASS) dataset~\cite{AMASS2019} and 3D pose in the wild (3DPW) dataset~\cite{vonMarcard2018}. They also used MPJPE to evaluate the distance between ground truth and the prediction result, commonly used to assess the human pose estimation.
Following the success of STS-GCN, A.~Bouazizi {\it et al.}~\cite{motionmixer2022} conducted the short and long-term human motion prediction using multilayer perceptron (MLP) architecture solely and achieved the best performance as of now. 

As a result, many improvements have been made to this problem with more datasets and cases. Even though 3D human motion forecasting has become an important issue to solve, getting the data of 3D joints in the real world is a different problem. Coming up with the idea of real-time human motion forecasting with an RGB camera, the 2D interpretation of data is unavoidable, which became the reason for conducting the 2D human motion forecasting research. In this research, the author proposed a new baseline for 2D human motion forecasting with Transformer-based architecture.


\subsubsection{2D Human Motion Forecasting}
One of the similar ideas of our research, single human motion forecasting, is conducted by predicting the human motion with 3D poses in the wild (3DPW) dataset~\cite{vonMarcard2018} for the 3D input dataset and Posetrack~\cite{posetrack} for the 2D input dataset.
However, due to the lack of the ground truth 2D input dataset, one could not compare using the Posetrack dataset.
Therefore, the author considered using the Human3.6M dataset and 3DPW dataset with 2D input as it provides more data and is broadly used in the human motion forecasting problem.

\subsubsection{Image Synthesis-based Forecasting}
Predicting the object not only for humans is also necessary, considering that humans are not the only object that can move in the real world.
An approach to predict pixel space for the following sequence of image features has been conducted~\cite{mathieu2015}.
It uses the convolutional neural network (CNN) to generate future frames from the input sequences.
The model can predict two frames with a structured similarity index to measure the similarity between the prediction and the ground truth images and sharpness evaluation to measure the loss of sharpness between the true frame and the prediction. Compared to our idea, they presented the method for a video not only with a specific object like in our research.
However, developing this video prediction for the future image is exciting and promising to be the next step of our research by expanding the object not only for human motion. 


\subsection{Transformer Network for Time Series Problem}
Since the attention-based method was introduced, the application of the attention-based method also broadens up not only used in NLP problems but also in time-series data prediction and classification tasks like image and object recognition. It has been proved to give a slight to significant improvement in the results. As for the example usage of the transformer network with time-series data, research to forecast the influenza prevalence case has been conducted~\cite{neo2020}.
Compared with the other time-series method like LSTM and seq2seq, which uses attention, the transformer model showed performance improvement in Pearson correlation and root mean square error.
As a result of this research, it can learn complex dependencies of various lengths from time-series data. Following the success of the transformer network for the time-series problem, the author considered using the transformer network for a more complex time-series problem like human motion forecasting. 


\section{Preliminaries}
\subsection{Dataset}
In other related research, the Human3.6M dataset is used in all the studies. This dataset is broadly used in human pose-related research such as the human pose estimation~\cite{zanfir2018, iskakov2019}. It was considering the gestures and features of the human body that are nicely provided by the Human3.6M dataset~\cite{human3.6m}. This dataset contains 17 scenarios by 11 subjects taken from 4 different angles with high-resolution 50~fps video. The Human3.6M dataset provided the scenarios in the studio-like capture with no interaction with another object. 3DPW dataset provided the real-life captured dataset with multiple human features in the frame\cite{vonMarcard2018}. As well as the Human3.6M dataset, the 3DPW dataset provided the 2D pose annotation data, which is suitable for this research. 


\subsection{Multi-Head Attention}
Multi-head attention is a module for attention mechanisms that run through an attention mechanism several times in parallel~\cite{vaswani2017}. Multiple attention heads allow for attending to parts of the sequence differently.

\subsection{Transformer Networks}
Transformer networks were proposed by Vaswani {\it et al.} for the machine translation tasks~\cite{vaswani2017}. It became state of the art for the NLP problems with the large scale of usage. The usage of the transformer networks was also broadened to solve the problem of the computer vision tasks by Vision-Transformer, which is a simplified model of transformer networks.
In the transformer networks used in the NLP, the attention mechanism tried to compute the relation between words in the sentence to be analyzed.
In the case of the vision transformer, it tried to calculate the different parts of the image.
By splitting the image into fixed-size patches,
linear embedding, adding positional embeddings, and feeding the resulting sequence of vectors to a standard transformer encoder,
then for the classification, the standard MLP head is used.

\section{Proposed Method}
In this section, the author describes the detail of the proposed method, which estimates the human body's key points (from now on called key points) in future $T_{P}$ frames from key points from past $T_{Q}$ frames by using the time series self-attention model. The overview of the method's flow is described in \textbf{Fig.~\ref{fig:methodsoverview}}.

\begin{figure}
    \centering
    \includegraphics[width=8cm]{./3/figures/flow_diagram.eps}
    \caption{Overview of the prediction flow. $T_Q$ number of input frames is defined and predict frame $T_P$ ahead of prediction. While in this research, the $T_P$ is defined as 10 for 400ms and 25 for 1000ms prediction task. }
    \label{fig:methodsoverview}
\end{figure} 


\subsection{Frame-by-frame 2D Pose Estimation}
Requiring the feature of key points from the input video for the processing, the 2D pose estimation is needed to interpret the image into the coordinate location of the key points.
In this research, the author used OpenPose as the standard method to estimate the 2D human pose in an image.
However, other methods are likewise applicable for this task, such as XNect~\cite{XNect}, ViTPose~\cite{vitae}, and ViTPose V2~\cite{vitaev2}.

% data preparation
Most 2D humans pose estimation methods on video images estimate the joint coordinates frame-by-frame,
and key points are expected to be the 2D data consisting of $x$- and $y$-coordinate in the video frame.
The estimated key points in the $i$-th video frame $\mathbf{Q}_{i} \in \mathbb{R}^{2 N}$ consists of $x$- and $y$-coordinates of $N$ points.
When the number of given frames is $T_Q$,
$\mathbf{Q} = \{ \mathbf{Q}_{1}, \mathbf{Q}_{2}, \ldots, \mathbf{Q}_{T_Q} \}$ is obtained as the sequence of feature vectors.

Notably, $\mathbf{Q}_{i}$ is flattened to a 1-dimensional vector, and its components are normalized by scaling between 0 and 1.


\subsection{Time Series Self-Attention}
This section describes the self-attention-based method to forecast key points in future $T_{P}$ frames from the input sequence $\mathbf{Q}$.
Transformer models apply the embedding to transform the raw data to the 1-D vector.
However, the input sequence in this research has the shape of a 1-D vector, which no longer needs the embedding transformation. The positional encoding is applied to the input data's positional index before feeding the input data to the transformer encoder and MLP head.

Considering that the input sequence $\mathbf{Q}$ is already a vector with numerical numbers, the embedding module is not needed in the network.
Positional encoding is added to the input to capture the positional information.
However, this positional encoding captures the position based on each time or frame input, as the position of the key points on each frame is fixed.
With this in mind, the model only considers the frame position.
The transformer encoder network computes the key points inside the frame regarding the frame position.
Let $t$ be the desired position in an input frame, $\overrightarrow{F_{t}} \in \mathbb{R}^d$ be its corresponding encoding, while $d$ is the encoding dimension which in this case $d = 32$. 

\begin{equation}\label{eq:3_posenc}
    \overrightarrow{F_{t}}^{(i)} = \begin{cases}
        \sin (\omega_{k} \cdot f) & \text{ if $i = 2k$}, \\
        \cos (\omega_{k} \cdot f) & \text{ if $i = 2k + 1$},
    \end{cases}
\end{equation}
where 

\begin{equation}\label{eq:3_posenc_weight}
    \omega_{k} = \frac{1}{ 10000^\frac{2k}{d}}
\end{equation}

and  $k$ is indices containing $\{0, 1, \dots, \frac{d}{2} - 1\}$.

As shown in \textbf{Fig.~\ref{fig:modeloverview}},
similar to the Transformer Encoder~\cite{vaswani2017},
the transformer encoder block in the proposed model includes the Multi-Head Attention, the MLP head block that contains the linear transformation in the fully-connected network, and the normalization layer applied after every block.
The MLP head block contains two linear layers with a ReLU activation function after each.
\begin{figure}
    \centering
    \includegraphics[width=15cm]{./3/figures/model_overview.pdf}
    \caption{Time series self-attention network. $L$ is the number of Transformer Encoder layers.}
    \label{fig:modeloverview}
\end{figure} 
The model with a linear layer dimension of 1,024 or 4,096 is set to examine the best model for each motion.
The expected output of the MLP head block is
$\mathbf{\hat{P}} = \{ \mathbf{\hat{P}}_1, \mathbf{\hat{P}}_2, \ldots, \mathbf{\hat{P}}_{T_{P}} \}$
, where $T_\mathrm{P}$ is the length of the expected output dimension.


\subsection{Loss Metric}
Root mean square error (RMSE) is employed to evaluate the distance of estimated key points sequence
$\mathbf{\hat{P}} = \{ \mathbf{\hat{P}}_1, \mathbf{\hat{P}}_2, \ldots, \mathbf{\hat{P}}_{T_{P}} \}$
from its corresponding ground truth sequence
$\mathbf{P} = \{ \mathbf{P}_1, \mathbf{P}_2, \ldots, \mathbf{P}_{T_{P}} \}$.
\begin{equation}
    \label{eq:rmse_all}
    \mathcal{L}_1 =
        \sqrt{
            \frac{1}{T_{p} N}
            \sum_{
                \mathbf{P}_i,
                \mathbf{\hat{P}}_i
            }
            \| \mathbf{P}_i - \mathbf{\hat{P}}_i \|^2
        }
\end{equation}
Additionally, because key points in the leg and arm have more significant movement than those in the body trunk, additional errors on those points are computed.

\begin{equation}
    \label{eq:rmse_add}
    \mathcal{L}_2 =
            \frac{1}{T_p \mathcal{N}}
            \sum_{
                \mathcal{P}_i,
                \mathcal{\hat{P}}_i
            }
            \| \mathcal{P}_i - \mathcal{\hat{P}}_i \|^2
\end{equation}
where $\mathcal{P}_i$ and $\mathcal{\hat{P}}_i$ is the set of left and right side of the shoulder, elbow, hand, hip, knee, and ankle key points in $\mathbf{P}_i$ and $\mathbf{\hat{P}}_i$, respectively.
$\mathcal{N}$ is the number of key points in $\mathcal{P}_i$.
Finally, loss function $\mathcal{L}$ is formulated as the weighted summation of $\mathcal{L}_1$ and $\mathcal{L}_2$.
\begin{equation}
    \mathcal{L} = \mathcal{L}_1 + w \mathcal{L}_2
\end{equation}
where $w$ is a weight parameter set to 4 in the experiment.


\section{Experiments}
\subsection{Dataset}
The Human3.6M dataset has been used with the 2D data pose representation from subjects and motions in this research.
The interpretation of 2D data from the Human3.6M dataset is provided by human pose estimation research~\cite{Zhao2019}.
The data is divided based on the training and testing.
The training data consists of subject numbers 1, 6, 7, 8, 9, and 11, and the testing data consists the subject number 5.
This setup is referred to as the related experiment~\cite{martinez2017}.
However, the evaluation methods and metrics apply to this research. As for the 3DPW dataset, the author follows the setting of the training, validation, and testing sets originally from the dataset itself~\cite{vonMarcard2018}.

For the evaluation of robustness against the input sequence $\mathbf{Q}$,
the testing accuracy is compared to ground truth provided by the Human3.6M dataset and OpenPose estimation.
In the ground truth testing, the ground truth key points are used for both the input sequence $\mathbf{Q}$ and output sequence $\mathbf{P}$.
In the OpenPose testing, the input sequence $\mathbf{Q}$ is given by OpenPose. Furthermore, only the ground truth output sequence $\mathbf{P}$ is used for calculating the evaluation metrics.
\subsection{Experimental Setup}
The author set up the model with several parameters, including the number of transformer encoder layers $L=6$, dropout value of 0.5, batch size of 64, and dimension on the linear layer of 1024 and 4096 to compare the result repeated in 5,000 epochs. 


The proposed model is trained and evaluated using a sliding window strategy in the time axis.
As shown in \textbf{Fig.~\ref{fig:sliding_window}}, one window consists of $T_Q$ input frames and $T_P$ output frames.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./3/figures/sliding_window.eps}
    \caption{
        Sliding window on the input and ground truth data as the expected output.
        Given the source input for the method with the shape of $T_{Q}$ frames and the target expected output with the shape of $T_{P}$ frames.
        One shifting scheme as the input is used to keep the input length sufficient for the model.
    }
    \label{fig:sliding_window}
\end{figure}

% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=\linewidth]{./3/figures/training.eps}
%     \caption{
%         Training loss on Human3.6M dataset ``Walking'' motion for long-term prediction task in 1000 sequences of epochs by RMSE loss metric $\mathcal{L}$.
%     }
%     \label{fig:training_loss}
% \end{figure}
In this research, the input data is determined to be 25 frames. The ground truth data is the subsequent 10 frames for the short-term prediction or 25 frames for the long-term prediction consecutively with one frame shifted.

The optimal batch size is 64,
and the optimal number of transformer encoder layers is $L=6$ for our environment.
The model took more memory and made the training process heavier if the batch size exceeded 64. 

The training process is done by specific single motions one by one with a learning rate of 0.001 with the Adam optimizer. Then, the sequence data with 25 frames is defined as the input, expecting 10 frames of 400 milliseconds or 25 frames of one second in the video.
The following experiments use the PyTorch environment on an NVIDIA GeForce RTX 3090 GPU. 

% \textbf{Figure \ref{fig:training_loss}} shows the training stability of the model with 5000 epochs for the long-term prediction task on the Human3.6M dataset ``Walking'' motion. The model performed well in the training process with a significant loss reduction below 1000 epochs. 


\subsection{Evaluation Metrics}\label{3:2D_evaluation}
Recent works in human pose forecasting and estimation have standardized the evaluation metrics by calculating the mean per-joint position error (MPJPE)~\cite{fragkiadaki2015recurrent, martinez2017, jain2016, wang2019, chiu2019, Mao2019, Sofianos2021, motionmixer2022}, mean per-joint velocity error (MPJVE)~\cite{Zhao2019, Pavllo2019} and mean per-joint localization error (MPJLE)~\cite{human3.6m}. MPJPE is calculated by computing the squared Euclidean distance between the ground truth and the prediction with respect to the treated joints.
The evaluation metric of MPJPE is defined by:
\begin{equation}\label{eq:mpjpe}
    \mathrm{MPJPE} =
        \frac{1}{N}
        \sum_{i=1}^{N} 
        \| \mathbf{P}_{i} - \mathbf{\hat{P}}_{i} \|,
\end{equation}
where $\mathbf{P}_{i}$ and $\hat{\mathbf{P}}_{i}$ is the ground truth and predicted coordinates in the frame $i$ in the $N$ frames respectively.

The MPJVE is calculated by computing the L2-norm of motion velocity, which is the one-frame difference of coordinates between the prediction and the ground truth.
The evaluation metric of MPJVE is defined by:
\begin{equation}
    \label{eq:mpjve}
    \mathrm{MPJVE} =
        \frac{1}{N}
        \sum_{i=1}^{N}
        \| \mathbf{V}_{i} - \mathbf{\hat{V}}_{i} \|,
\end{equation}
where $\mathbf{V}_{i}$ and $\mathbf{\hat{V}}_{i}$ are velocity from frame $i-1$ to time $i$, and is defined by:
\begin{equation}
    \label{eq:mpjve_velocity}
    \mathbf{V}_{i} = \mathbf{P}_{i} - \mathbf{P}_{i - 1}.
\end{equation}

If MPJPE is defined to calculate the distance from the prediction to the ground truth and MPJVE is to define the mean differences in each frame movement. MPJLE defines the localization of the correct key point within the tolerance value. The evaluation metric of MPJLE is defined by:
\begin{equation}\label{eq:mpjle}
    \mathrm{MPJLE} =
        \frac{1}{N} 
        \sum_{i=1}^{N}
        \mathds{1}_{\| \mathbf{P}_{i} - \mathbf{\hat{P}}_{i} \| \ge t},
\end{equation}
where $\mathds{1}$ is a binary step function that gives 0 if the distance value is within $t$ and gives 1 otherwise. At the same time, $t$ is the integral tolerance value in an interval. In this case, the author defined $t = [0, 200]$. One can obtain an estimate of the average error. In the same way, the mean average precision measure the performance of a classifier~\cite{human3.6m}.

Qualitatively, the skeleton data is shown to visualize the difference between the prediction and the ground truth.
Additionally, to know the computational time of the proposed model on each frame to produce the prediction value, the average time taken by motion is calculated.
This is necessary considering predicting human activity for the role in real life.

\section{Results}
As described in Section~\ref{3:2D_evaluation} for the 2D human motion forecasting task, the prediction result is quantitatively evaluated by the evaluation methods to show that the prediction is correct or sufficient based on the distance of the prediction result and the corresponding ground truth data. As for the comparison in the 2D human motion prediction, the state-of-the-art method, such as LSTM and GRU, is used to compare the validity of our method.
Qualitatively, the prediction result could be seen in frames in the video comparing the ground truth and prediction key point skeleton movement.

\subsection{Model Training}
In this section, the author describes the evaluation of the model in the training phase. To show the model validity of learning the certain problem in the data, evaluation over the training phase is needed. \textbf{Fig.~\ref{fig:2D_training_walking_long}} shows the training phase on the Walking motion using the Human3.6M dataset. The RMSE with additional weight is computed to evaluate the distance from the prediction results to the corresponding ground truth.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./3/figures/training.eps}
    \caption{
        Our model is trained for 1000 epochs using Walking motion data for the long-term prediction task.
    }
    \label{fig:2D_training_walking_long}
\end{figure}


\subsection{Quantitative Evalulation}\label{4:2Dquantitative}
The results are separated regarding the dataset and evaluation metric for quantitative evaluation.
\subsubsection{Human3.6M Dataset}\label{4:2Dhuman3.6}


\textbf{Comparison based on MPJPE.} In this part, the prediction results are evaluated based on the MPJPE score to see the distances from the prediction to the ground truth.
\textbf{Table. \ref{tbl:2DMPJPE_gt_short}} shows the evaluation result based on the MPJPE for the Human3.6M dataset comparing the time series self-attention method (hereinafter called Ours), RNN-LSTM, and RNN-GRU in the short-term prediction task. Our method is performed with two linear dimension values, 1024 and 4096, to see the difference in which dimension gives the best results. 

\begin{table}
    \centering
    \caption{MPJPE of 2D joint positions in pixel on the Human3.6M dataset using the real position data as the testing data for the short-term prediction task.}
    \begin{tabular}{l|rrrr}
        \toprule
        \multirow{2}{*}{\textbf{Motion}} & \multicolumn{4}{c}{400 msec} \\
            & \textbf{Ours (1024)} & Ours (4096) & LSTM & GRU  \\
        \midrule
        Walking	&	\textbf{13.32}	&	49.19	&	14.38	&	13.90\\
Eating	&	\textbf{8.51}	&	12.57	&	13.14	&	14.48	\\
Smoking	&	9.91	&	\textbf{7.51}	&	16.47	&	17.40	\\
Discussion	&	11.20	&	\textbf{8.55}	&	20.62	& 20.75 \\
Direction	&	28.09	&	\textbf{7.63}	&	21.55	& 21.77 \\
Greeting	&	15.26	&	\textbf{14.38}	&	36.67	& 35.42 \\
Phoning	&	60.26	&	\textbf{11.20}	&	19.00	&	18.89 \\
Waiting	&	37.08	&	70.17	&	26.98	&	\textbf{25.84} \\
Walking Dog	&	\textbf{10.97}	&	29.93	&	41.19	&	39.60 \\
Walking Together	&	\textbf{9.09}	&	32.48	&	45.42	&	46.32	\\
Posing	&	47.98	&	\textbf{8.19}	&	30.76	&	27.86	\\
Sitting	&	\textbf{9.99}	&	19.28	&	18.91	&	21.74	\\
Sitting Down	&	\textbf{8.37}	&	23.09	&	27.95	&	28.84 \\
Taking Photo	&	59.11	&	119.23	&	36.58	&	\textbf{34.52}	\\

        \midrule
        Average	&	\textbf{23.51} 	&	29.53	&	26.40	&	26.24	\\
        \bottomrule
    \end{tabular}
    \label{tbl:2DMPJPE_gt_short}
\end{table}

The best result can be seen in the bold highlighted value. Our method with 1024 linear dimensions obtained the best result compared to the other method in Walking, Eating, Walking with Dog, Walking Together, Sitting, and Sitting Down motions. This means 6 over 14 motions, or 42.8 percent of the motions with a range of the MPJPE score around 8 to 14 pixels, within the best-predicted motions with our method with 1024. At the same time, Our method with a linear dimension of 4096 obtained the best result on Smoking, Discussion, Direction, Greeting, Phoning, and Posing motions. Similar to the linear 1024, this model also obtained 6 over 14 motions or 42.8 percent of the motions with a range of error of around 7 to 15 pixels. However, our method failed to predict the Waiting and Taking Photo motions with quite a significant MPJPE score of around 37 to 120 pixels. Instead of our model, GRU could predict the motion better with an MPJPE score of 25.84 pixels on Waiting and 34.52 pixels on Taking Photo motion. Indeed 25 to 35 pixels is quite a significant score compared to the other best results. With regard to this MPJPE score, Waiting and Taking Photo motions are considered the most challenging case, as these motions are aperiodic motions which means that these motions are not recurring at regular intervals.

In contrast, Our model with 1024 linear dimensions obtained bad results on Direction, Phoning, and Posing motions compared to the other models with a range of 28 to 61 pixels. On the other hand, our model with 4096 linear dimensions obtained bad results on Walking, Walking Dog, and Walking Together compared to the other models with a range of 29 to 50 pixels. 

\begin{table}
    \centering
    \caption{MPJPE of 2D joint positions in pixel on the Human3.6M dataset using the real position data as the testing data for the long-term prediction task.}
    \begin{tabular}{l|rrrr}
        \toprule
        \multirow{2}{*}{\textbf{Motion}} & \multicolumn{4}{c}{1000 msec} \\
            & \textbf{Ours (1024)} & Ours (4096) & LSTM & GRU  \\
        \midrule
        
        Walking	&	14.43	&	\textbf{12.03}	&	12.11	&	12.74\\
Eating	&	\textbf{9.11}	&	10.11	&	14.09	&	13.02\\
Smoking	&	\textbf{7.11}	&	8.27	&	15.21	&	16.53\\
Discussion	&	\textbf{8.34}	&	14.30	&	21.21	&	19.48\\
Direction	&	\textbf{5.76}	&	8.17	&	20.35	&	20.56\\
Greeting	&	12.90	&	\textbf{10.67}	&	33.92	&	32.56\\
Phoning	&	\textbf{9.11}	&	10.85	&	18.41	&	16.93\\
Waiting	&	\textbf{6.37}	&	7.15	&	24.46	&	22.37\\
Walking Dog		&	\textbf{10.98}	&	13.1	&	39.99	&	38.82\\
Walking Together	&	\textbf{8.15}	&	8.20	&	40.82	&	29.62\\
Posing	&	15.46	&	\textbf{8.41}	&	26.38	&	25.96\\
Sitting	&	\textbf{9.61}	&	17.05	&	20.73	&	19.48\\
Sitting Down	&	9.18	&	\textbf{7.59}	&	31.25	&	25.74 \\
Taking Photo	&	17.69	&	\textbf{12.53}	&	40.70	&	29.50\\

        \midrule
        Average	&	\textbf{10.30}	&	10.60	&	25.69	&	23.09\\
        \bottomrule
    \end{tabular}
    \label{tbl:2DMPJPE_gt_long}
\end{table}

According to these results, our model with a linear dimension of 1024 could not predict the motions with not so much movement but good with the recurrent motions. In contrast, our model with a linear dimension of 4096 obtained bad results on the motions with a lot of movement but could predict well on the motions with not so much movement. This means that more dimension linear gives the effect of more possibilities and a better understanding of passively moving actions, but many possibilities value also reduces the context understanding of the actively moving actions.
On average, our model, with 1024 linear dimensions, obtained the best result compared to other models with 23.51 pixels by MPJPE. However, the average values were not very different from the other models due to some significant errors.

\textbf{Table. \ref{tbl:2DMPJPE_gt_long}} shows the MPJPE evaluation result of the long-term human motion prediction task on the Human3.6M dataset. While on the short-term prediction showing, the prediction results varied based on the motions. Long-term prediction results show consistency over all motions in the range of 5 to 18 pixels by our models. On average, our model with a linear dimension of 1024 obtained the best result of 10.30 pixels by MPJPE. Followed by a very small difference in our model with a linear dimension of 4096 by 10.60 pixels on average of MPJPE. The best prediction results were obtained on the Direction motion with 5.76 pixels by MPJPE score. There is not so much difference in our model with a linear dimension of 1024 and 4096 in the long-term prediction task, which indicates the model could predict well in any case of the motions. Our model outperformed the RNN-based method with quite a significant MPJPE score. The motion-wise comparison shows our model with a linear dimension of 1024 is a bit bigger than the other models, while the other prediction results on other motions are smaller than the RNN-based models. 

Given a clean annotation over the key points of the human body poses might be unrealistic in the real world yet. Due to this reason, the pose estimation key point detection is used to extract the human body pose features as the testing data for our trained model. In this case, OpenPose is used to generate the human body pose features as it is currently one of the most used pose estimation methods. The features with the noise of incorrect estimation could be one barrier for the model to predict future human motion. With this in mind, our model could be evaluated with the noisy data obtained from the real-time human pose estimation. 

\begin{table}
    \centering
    \caption{MPJPE of 2D joint positions in pixel on the Human3.6M dataset using human pose estimation by OpenPose as the testing data.}
    \begin{tabular}{l|rr|rr}
    
        \toprule
        Forecasting ({\textit msec})  & \multicolumn{2}{c|}{400}                        & \multicolumn{2}{c}{1000} \\
         Model    & \textbf{Ours (1024)}  &  Ours (4096) & \textbf{Ours (1024)}  &  Ours (4096)\\

        \midrule
        Walking	 & \textbf{29.10} & 52.38  & \textbf{25.82} & 36.00\\
        Eating & \textbf{22.51} & 26.73 & 24.69  & \textbf{16.79}\\
        Smoking & 50.50 & \textbf{38.53} & 21.12  & \textbf{19.15}\\
        Discussion & 26.75 & \textbf{21.02} & \textbf{21.66}  & 25.94\\
        Direction & 26.24 & \textbf{19.40} & \textbf{17.43} & 19.53\\
        Greeting & 29.11 & \textbf{25.09}  & 23.18 & \textbf{16.80}\\
        Phoning & 61.17 & \textbf{24.84} & 22.47 & \textbf{21.39}\\
        Waiting & \textbf{35.05} & 70.17 & \textbf{23.96} & 25.34\\
        Walking Dog & \textbf{25.70} & 48.98 & \textbf{23.27} & 83.43\\
        Walking Together & 48.62 & \textbf{39.19} & \textbf{23.01} & 23.51\\
        Posing & 58.16 & \textbf{23.84} & 27.80 & \textbf{23.91}\\
        Sitting & \textbf{17.27} & 18.91 & 22.29 & \textbf{20.87}\\
        Sitting Down & \textbf{22.97} & 29.18 & 21.05 & \textbf{17.28}\\
        Taking Photo & \textbf{78.84} & 119.23 & 23.80 & \textbf{22.73}\\
        \midrule
        Average  &  \textbf{37.71} &  39.90 & \textbf{22.97} &  26.62\\
        \bottomrule
    \end{tabular}
    \label{tbl:MPJPE_openpose}
\end{table}

\textbf{Table.~\ref{tbl:MPJPE_openpose}} shows the MPJPE score evaluation on the data generated by OpenPose as the pose estimation on the Human3.6M dataset. For the short-term prediction task, our model with 1024 linear dimensions obtained the best result on average, with a small difference over our model with a linear dimension of 4096. As the comparison regarding the motions, our model with a linear dimension of 1024 obtained quite consistent MPJPE scores with a range of 17 to 35 pixels on most of the motions. Expect the Smoking, Phoning, Walking Together, Posing, and Taking Photo motions. Comparing the prediction results on the ground truth annotation testing, the Phoning, Posing, and Taking Photo motions also obtained bad results. The difference between the result of the ground truth annotation testing and the OpenPose extracted features testing is 14.49 pixels, which indicates the prediction result by using the OpenPose generated features is still reliable to get the future of human body motions.

Similarly, our model with a linear dimension of 4096 obtained almost the same pattern as the evaluation result on the ground truth annotation data testing with only 10.29 pixels differences between the evaluation result of the OpenPose generated features testing. 

Furthermore, on the long-term prediction task, our models predict better regarding the MPJPE evaluation score. The results of both models consistently predict human motion in a range of 16 to 36 pixels of MPJPE score. However, our model with a linear dimension of 4096 failed to predict future human motion with the MPJPE score of 83.43 pixels. As a result, our model with 1024 and 4096 linear dimensions can predict future human motion on average. Even though the MPJPE score is quite big, the prediction result is still reliable for predicting the human location movement, while on the other hand, the model could not predict to visualize the human body pose well.

Additionally, \textbf{Fig.~\ref{fig:2D_mpjpe_keypoints}} shows the MPJPE evaluation on the key points with respect to the motions. The ankle's key points in the Walking motion obtained the highest MPJPE score compared to the difference with another key point in the other motions. Since the ankles are the most moving key points in the Walking motion, followed by the hands and elbows. This could be explained more clearly in the qualitative evaluation in Section~\ref{4:2Dqualitative}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{./4/figures/result_keypoints_long_1024.eps}
    \caption{
        MPJPE-based evaluation on the key points and motions for the long-term prediction task using our model with a linear dimension of 1024. The heatmap color contains the MPJPE score based on the color scale on the right side.
    }
    \label{fig:2D_mpjpe_keypoints}
\end{figure}

For more details, \textbf{Fig.~\ref{fig:2D_openposegt_motions}} shows the comparison between the testing result on the data obtained by OpenPose and the ground truth testing in each motion.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{./3/figures/allmotion_boxplot_1024_long.pdf}
    \caption{
        Comparison of MPJPE distance for each motion on data obtained by OpenPose and data from the dataset for testing.
    }
    \label{fig:2D_openposegt_motions}
\end{figure}

\textbf{Figure \ref{fig:2D_mpjpe_frame}} shows the comparison of the MPJPE distance trajectories on the OpenPose testing and ground truth testing by each frame in the ``Walking'' motion.
\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{./3/figures/graph_walking_mpjpe_frame_long.pdf}
    \caption{
        Comparison of MPJPE in long-term prediction by frame on OpenPose testing and ground truth testing with linear 1024 model.
    }
    \label{fig:2D_mpjpe_frame}
\end{figure}


\textbf{Comparison based on MPJVE.} In this part, the evaluation based on MPJVE is described in detail to compare the smoothness of the movement prediction results obtained by the methods.

% mpjve
\begin{table}
    \centering
    \caption{MPJVE of 2D joint positions in pixel on the Human3.6M dataset using the real position data as the testing data.}
    \begin{tabular}{l|rrrr}
        \toprule
        \multirow{2}{*}{Motion}  & \multicolumn{4}{c}{400 msec} \\
            & Ours (1024) & \textbf{Ours (4096)} & LSTM & GRU \\
        \midrule
Walking	&	\textbf{1.05}	&	1.67	&	1.15	&	1.23	 \\
Eating	&	\textbf{0.70}	&	0.73	&	0.97	&	1.00	\\
Smoking	&	2.7	&	1.45	&	\textbf{0.86}	&	0.89	 \\
Discussion	&	1.34	&	1.28	&	\textbf{1.27}	&	1.33	\\
Direction	&	1.34	&	\textbf{0.69}	&	1.15	&	1.23	\\
Greeting	&	1.33	&	\textbf{1.30}	&	2.77	&	2.91	 \\
Phoning	&	2.38	&	\textbf{0.83}	&	1.11	&	1.10	 \\
Waiting	&	2.59	&	\textbf{0.93}	&	1.60	&	1.62	 \\
Walking Dog	&	4.81	&	2.79	&	2.51	&	\textbf{2.38}	\\
Walking Together	&	1.28	&	\textbf{1.25}	&	2.36	&	2.35	 \\
Posing	&	1.64	&	\textbf{0.84}	&	1.58	&	1.60	\\
Sitting	&	0.85	&	\textbf{0.9}	&	1.18	&	1.44	\\
Sitting Down	&	2.69	&	1.64	&	1.30	&	\textbf{1.21}	 \\
Taking Photo	&	3.15	&	\textbf{0.73}	&	1.31	&	1.24	 \\

        \midrule
        Average	&	1.99	&	\textbf{1.22}	&	1.51	&	1.54	\\
        \bottomrule
    \end{tabular}
    \label{tbl:2DMPJVE_gt_short}
\end{table}

\textbf{Table. \ref{tbl:2DMPJVE_gt_short}} shows the score of MPJVE evaluation obtained by our model with a linear dimension of 1024 and 4096, RNN-LSTM, and RNN-GRU for the short-term prediction task. Based on the motions, empirically, the motion with more movement will obtain more MPJVE scores due to the changes in the movement over the frames. For example, comparing the Eating and Walking Dog motion will obtain a very different MPJVE score since most of the Eating motion movements stayed on the same spot while the only movement was at the hands and some gestures of the torso. On average, our model with a linear dimension of 4096 obtained the best MPJVE score with 1.22 pixels over other methods. Our model with a linear dimension of 1024 could predict the best based on the MPJPE score, but based on the MPJVE, the prediction result is not as smooth as the other method obtained. This indicates the results obtained by this model are quite spiky, with 1.99 pixels average movement for 1 frame. At the same time, the worst result was obtained by our model with a linear dimension of 1024 on the Walking Dog motion with 4.81 pixels average movement for 1 frame step. On the other hand, the prediction results obtained by the RNN-based models are quite consistent with the range of the MPJVE score of 0.8 to 2.5 pixels. 

\textbf{Table. \ref{tbl:2DMPJVE_gt_long}} shows the score of MPJVE evaluation obtained by our model with a linear dimension of 1024 and 4096, RNN-LSTM, and RNN-GRU for the long-term prediction task. On average, both of our models outperformed the RNN-based method. Our model achieved the best results when using a linear dimension of 4096, and it obtained the highest MPJVE score for 11 out of 14 motions. Additionally, our model performed well for the remaining 3 out of 14 motions using a linear dimension of 1024. All the models demonstrated exceptional performance in the long-term prediction task, with an MPJPE score range of 0.6 to 2.39 pixels for a 1-frame step. Overall, our approach showed strong performance in both short-term and long-term prediction tasks. In the short-term prediction task, using a larger linear dimension resulted in better performance. In contrast, for the long-term prediction task, there was only a minimal difference in performance between using a linear dimension of 1024 and 4096, with an MPJPE score difference of only 0.1 pixels.

% mpjve
\begin{table}
    \centering
    \caption{MPJVE of 2D joint positions in pixel on the Human3.6M dataset using the real position data as the testing data.}
    \begin{tabular}{l|rrrr}
        \toprule
        \multirow{2}{*}{Motion}                            & \multicolumn{4}{c}{1000 msec} \\
            & Ours (1024) & \textbf{Ours (4096)} & LSTM & GRU \\
        \midrule
        Walking	&	1.11	&	\textbf{1}	&	1.07	&	1.22 \\
Eating	&	0.71	&	\textbf{0.66}	&	1.04	&	1.10 \\
Smoking	&	0.66	&	\textbf{0.62}	&	0.84	&	0.88 \\
Discussion	&	\textbf{1.16}	&	1.36	&	1.31	&	1.24 \\
Direction	&	\textbf{0.72}	&	0.77	&	1.15	&	1.16 \\
Greeting	&	1.45	&	\textbf{1.26}	&	2.39	&	2.67 \\
Phoning	&	0.87	&	\textbf{0.80}	&	1.11	&	1.14 \\
Waiting	&	0.98	&	\textbf{0.85}	&	1.45	&	1.62 \\
Walking Dog	&	1.28	&	\textbf{1.1}	&	2.16	&	2.37 \\
Walking Together	&	0.72	&	\textbf{0.67}	&	2.00	&	2.03 \\
Posing	&	1.31	&	\textbf{0.91}	&	1.58	&	1.67 \\
Sitting	&	1.07	&	\textbf{0.9}	&	1.41	&	1.29 \\
Sitting Down	&	0.97	&	\textbf{0.76}	&	1.61	&	1.53 \\
Taking Photo	&	\textbf{0.88}	&	0.83	&	1.40	&	1.33 \\
        \midrule
        Average	&	0.99	&	\textbf{0.89}	&	1.47	&	1.52\\
        \bottomrule
    \end{tabular}
    \label{tbl:2DMPJVE_gt_long}
\end{table}

During the evaluation, models were compared to ground truth annotation data in testing. Additionally, models were tested on non-annotated data that had been processed with the OpenPose feature extraction method as shown in the \textbf{Table. \ref{tbl:MPJVE_openpose}}. The evaluation results using non-annotated data for testing yielded significantly higher average movement over 1 frame step than when ground truth annotated data was used. The average movement when using non-annotated data was 5 to 9 pixels, whereas it was only between 0.8 to 2 pixels when using annotated data. However, it's important to note that comparing the results on average may not provide an accurate representation of the performance as the different motions have distinct characteristics. For instance, in the case of the "Walking Together" motion, the MPJVE score is relatively higher than other motions because the subject in this motion has more movement than the other motions. This also suggests that the more movement occurs within a frame, the harder it is for the model to generate a smooth movement prediction over multiple frames.

\begin{table}
    \centering
    \caption{
        MPJVE of 2D joint positions on the Human3.6M dataset using human pose estimation by OpenPose as the testing data.
    }
    \begin{tabular}{l|rr|rr}
    \toprule
        Forecasting ({\textit msec})  & \multicolumn{2}{c|}{400}                        & \multicolumn{2}{c}{1000} \\
         Model    & Ours (1024)  &  \textbf{Ours (4096)} & \textbf{Ours (1024)}  &  Ours (4096)\\

        \midrule
        Walking & \textbf{7.92} & 9.79 & \textbf{6.89} & 7.82\\
        Eating & 4.87 & \textbf{3.63} & \textbf{4.85} & 4.92\\
        Smoking & 14.61 & \textbf{5.96} & 5.78 & \textbf{5.59}\\
        Discussion & 7.11 & \textbf{6.96} & 7.50 & \textbf{6.13}\\
        Direction & 5.20 & \textbf{4.82} & 5.29 & \textbf{4.84}\\
        Greeting & 8.12 & \textbf{8.07} & \textbf{1.45} & 7.34\\
        Phoning & 7.35 & \textbf{5.03} & 5.44 & \textbf{5.02}\\
        Waiting & 7.36 & \textbf{0.93} & 7.43 & \textbf{6.03}\\
        Walking Dog & \textbf{8.19} & 11.42 & \textbf{8.29} & 9.50\\
        Walking Together & 24.50 & \textbf{11.87} & \textbf{9.29} & 9.30\\
        Posing & 6.31 & \textbf{6.20} & \textbf{4.59} & 6.96\\
        Sitting & 3.74 & \textbf{3.44} & 3.34 & \textbf{2.84}\\
        Sitting Down & \textbf{5.72} & 5.78 & 5.86 & \textbf{5.33}\\
        Taking Photo & 11.98 & \textbf{0.73} & \textbf{4.34} & 5.05\\
        \midrule
        Average  & 8.79 & \textbf{6.05} & \textbf{5.74} & 6.19\\
        \bottomrule
    \end{tabular}
    \label{tbl:MPJVE_openpose}
\end{table}

\textbf{Comparison based on MPJLE.} In this part, the author describes the evaluation based on the MPJLE metric to show the localization of the prediction results by the Threshold at the tolerance $t$ as shown on \textbf{Table. \ref{tbl:2DMPJLE_hum36m}}. The tolerance $t$ is an interval from 0 to 200 pixels. When the prediction result is above the threshold of tolerance, the result is considered an error. The performance of the methods is evaluated at different thresholds (5, 10, 20, 50, 75, 100, 150, and 200). The best performance for each threshold is highlighted in bold. Overall, it appears that the "Ours (4096)" method performs the best, having the lowest MPJLE among all methods at most thresholds. Although the performance of our model with a linear dimension of 1024 is similar to that of our model with 4096 linear dimensions, at a threshold of 20 pixels, our model with a linear dimension of 1024 only slightly outperforms the latter, with a difference of just 0.02\%.

\begin{table}
    \centering
    \caption{MPJLE of 2D joint positions on the Human3.6M dataset in long-term prediction task.}
    \begin{tabular}{l|rrrrrrrr}
        \toprule
        \multirow{ 2}{*}{\textbf{Methods}}   & \multicolumn{7}{c}{\textbf{Threshold@$t$}} \\
          & \textbf{5} & \textbf{10} & \textbf{20} & \textbf{50} & \textbf{75} & \textbf{100} & \textbf{150} & \textbf{200} \\
          \midrule
        LSTM	&	1.000	&	1.000	&	1.000	&	0.884	&	0.654	&	0.446	&	0.225	&	0.121 \\
        GRU	&	1.000	&	1.000	&	1.000	&	0.854	&	0.627	&	0.435	&	\textbf{0.193}	&	\textbf{0.092} \\
        \textbf{Ours (1024)}	&	1.000	&	1.000	&	\textbf{0.980}	&	0.662	&	0.505	&	0.387	&	0.226	&	0.150 \\
        \textbf{Ours (4096)}	&	1.000	&	1.000	&	1.000	&	\textbf{0.635}	&	\textbf{0.457}	&	\textbf{0.373}	&	0.218	&	0.141 \\
        \bottomrule
    \end{tabular}
    \label{tbl:2DMPJLE_hum36m}
\end{table}

\subsubsection{3DPW dataset}\label{4:2D3dpw}
In this section, the author describes the result of evaluating the human motion prediction task using the 3DPW dataset by time series self-attention, RNN-LSTM, and RNN-GRU.
To evaluate the effectiveness of the methods when using different datasets, an evaluation was performed using the 3DPW dataset and based on the MPJPE metric as shown on \textbf{Table.~\ref{tbl:2DMPJPE_3dpw}}. While the Human3.6M dataset collected data at the indoor studio, the 3DPW dataset collected the outdoor data with unscripted motions. Hence, overall the MPJPE scores using the 3DPW dataset are very high. This means that the models could not predict future human motion. However, based on the evaluation result of MPJPE scores, the 1-layer RNN-GRU obtained the best average prediction result with 236.31 pixels. Our model could only be performed better than the RNN-based method on the left ankle key point with a very small difference compared to the RNN-GRU. 

%  3DPW dataset
\begin{table}
    \centering
    \caption{MPJPE of 2D joint positions in pixel on the 3DPW dataset using the real position data as the testing data. The author compared our method with LSTM and GRU models for Layer $L=1$, $2$, and $3$ with the 3DPW dataset.}
    \resizebox{\textwidth}{!}{\begin{tabular}{l|rrr|rrr|rrr}
        \toprule
        \multirow{2}{*}{Keypoints} & \multicolumn{3}{c|}{Our Method}  & \multicolumn{3}{c|}{LSTM} & \multicolumn{3}{c}{\textbf{GRU}} \\
         & $L=1$ & $L=2$ & $L=3$ & $L=1$ & $L=2$ & $L=3$ & $\textbf{L=1}$ & $L=2$ & $L=3$ \\
        \midrule
        Head	&	300.66	&	304.49	&	305.94	&	291.18	&	292.15	&	287.65	&	287.09	&	304.28	&	\textbf{283.75} \\
Neck	&	210.09	&	211.55	&	243.07	&	198.10	&	216.20	&	215.61	&	\textbf{193.15}	&	210.02	&	205.77 \\
Right Shoulder	&	230.72	&	219.34	&	254.70	&	208.71	&	223.54	&	224.08	&	\textbf{200.17}	&	218.20	&	216.35 \\
Right Elbow	&	272.43	&	258.64	&	289.76	&	243.50	&	260.86	&	259.78	&	\textbf{236.62}	&	242.08	&	251.56 \\
Right Arm	&	284.46	&	285.97	&	304.73	&	268.98	&	280.76	&	298.59	&	268.57	&	\textbf{260.71}	&	277.89 \\
Left Shoulder	&	225.60	&	226.63	&	252.11	&	209.95	&	228.16	&	232.55	&	\textbf{206.32}	&	222.86	&	216.89 \\
Left Elbow	&	262.93	&	251.46	&	279.75	&	252.70	&	260.94	&	275.28	&	\textbf{244.77}	&	259.65	&	249.14 \\
Left Arm	&	278.76	&	269.65	&	290.99	&	278.02	&	276.12	&	302.90	&	270.06	&	278.03	&	\textbf{268.75} \\
Right Hip	&	200.78	&	207.60	&	238.80	&	189.49	&	207.88	&	207.93	&	\textbf{187.28}	&	206.86	&	203.76 \\
Right Knee	&	201.87	&	212.06	&	246.68	&	205.82	&	213.77	&	215.71	&	\textbf{198.18}	&	210.54	&	214.54 \\
Right Ankle	&	232.04	&	243.93	&	269.89	&	237.51	&	245.93	&	244.32	&	\textbf{227.35}	&	239.76	&	244.73 \\
Left Hip	&	212.96	&	210.56	&	238.46	&	\textbf{191.73}	&	213.81	&	213.65	&	193.32	&	211.36	&	206.62 \\
Left Knee	&	215.32	&	218.37	&	243.52	&	208.82	&	230.06	&	220.52	&	\textbf{207.50}	&	222.62	&	216.19 \\
Left Ankle	&	\textbf{237.89}	&	245.12	&	272.82	&	244.08	&	266.75	&	252.06	&	239.56	&	257.20	&	246.73 \\
Right Eye	&	297.97	&	315.54	&	316.55	&	316.22	&	305.46	&	\textbf{292.54}	&	317.87	&	335.00	&	311.33 \\
Left Eye	&	328.79	&	347.23	&	318.94	&	310.22	&	\textbf{289.83}	&	309.99	&	303.15	&	319.58	&	303.48 \\

        \midrule
        Average	&	249.58	&	251.76	&	272.92	&	240.94	&	250.77	&	254.32	&	\textbf{236.31}	&	249.92	&	244.84 \\
        \bottomrule
    \end{tabular}}
    \label{tbl:2DMPJPE_3dpw}
\end{table}


\subsubsection{Computational Time}
Additionally, the author acknowledged the average computation time by the model to predict a frame of motion as an important note, considering that in the real world, one needs optimal computation to get a real-time prediction.
As shown on \textbf{Table~\ref{tbl:2Dcomputationtime}}, the model could generate the 400 milliseconds prediction around 2.4 seconds for a frame in general.
On average, the models with linear dimensions of 1024 and 4096 are not significantly different, with only 0.03 seconds differences.
The average computation time principally depends on the process and the power of the GPU, which is only comparable when using the same GPU with no other process running since it could affect the time taken by the GPU to generate the prediction.

\begin{table}
    \centering
    \caption{The average computation time in seconds for a frame to be predicted in the testing by the model.}
    \begin{tabular}[t]{l|c}
        \toprule
        Motion              & Processing time (\textit{sec})\\
        \midrule
        Walking             & 2.44\\
        Walking Dog         & 2.42\\
        Walking Together    & 2.42\\
        Discussion          & 2.43\\
        Eating              & 2.42\\
        Greeting            & 2.42\\
        Phoning             & 2.42\\
        Taking Photo        & 2.42\\
        Posing              & 2.42\\
        Sitting             & 2.42\\
        Sitting Down        & 2.43\\
        Smoking             & 2.43\\
        Waiting             & 2.43\\
        Direction           & 2.43\\
        \midrule
        Average             & 2.43\\
        \bottomrule
    \end{tabular}
    \label{tbl:2Dcomputationtime}
\end{table} 


\subsection{Qualitative Evalulation}\label{4:2Dqualitative}
In this section, the author describes the evaluation results in the qualitative-based comparison. 
\textbf{Fig.~\ref{fig:2D_short_qual}} shows the short-term prediction task results on Walking, Eating, Smoking, and Discussion motion. At the same time, the \textbf{Fig.~\ref{fig:2D_long_qual}} shows the long-term prediction task results on the same motions. The motions of the qualitative-based evaluation are shown with respect to the 5 frame steps to show the differences in moving motions. The blue line refers to the ground truth based on the annotated data from the corresponding frames, and the green line refers to the prediction results obtained by our models with a linear dimension of 1024. As shown in the figures, some poses failed to be precisely predicted. \textbf{Fig.~\ref{fig:2d_short_walking}} shows quite off from the correct pose in the 5 sequences poses. However, the location of the human is still correct regardless of the pose. At the same time, the 2 last poses seem to follow the pose. 
\textbf{Fig.~\ref{fig:2D_short_smoking}} shows the pose with a large error in the pose prediction. Our model failed to predict the pose well but could predict the location of the human regardless of the pose. This qualitative evaluation is in line with the quantitative evaluation on \textbf{Table.~\ref{tbl:2DMPJPE_gt_short}}.

The long-term prediction task performed better in most motions except for the walking motions compared to the short-term prediction task. This could be seen based on the qualitative evaluation results on the \textbf{Fig.~\ref{fig:2D_long_qual}} which is in line with the quantitative evaluation result on the \textbf{Table.~\ref{tbl:2DMPJPE_gt_long}}.

\textbf{Fig.~\ref{fig:2D_good_qual}} shows a good prediction result based on the qualitative comparison of the corresponding ground truth. Meanwhile, \textbf{Fig.~\ref{fig:2D_bad_qual}} shows the bad prediction result. On the good prediction results, our model could predict the pose very well, including the hands and legs movements which are considered the most challenging key points to predict since it moves much more than the other key points.

\begin{figure}
     \centering
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Short_qualitatives/Walkinghumanposediv.eps}
         \caption{Walking}
         \label{fig:2d_short_walking}
     \end{subfigure}
     % \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Short_qualitatives/Eatinghumanposediv.eps}
         \caption{Eating}
         \label{fig:2d_short_eating}
     \end{subfigure}
     % \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Short_qualitatives/Smokinghumanposediv.eps}
         \caption{Smoking}
         \label{fig:2D_short_smoking}
     \end{subfigure}
     % \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Short_qualitatives/Discussionhumanposediv.eps}
         \caption{Discussion}
         \label{fig:2D_short_Discussion}
     \end{subfigure}
     \caption{Short-term prediction result by our model using 1024 linear dimension in Human3.6M dataset.}
     \label{fig:2D_short_qual}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Long_qualitatives/Walkinghumanposediv.eps}
         \caption{Walking}
         \label{fig:2d_long_walking}
     \end{subfigure}
     % \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Long_qualitatives/Eatinghumanposediv.eps}
         \caption{Eating}
         \label{fig:2d_long_eating}
     \end{subfigure}
     % \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Long_qualitatives/Smokinghumanposediv.eps}
         \caption{Smoking}
         \label{fig:2D_long_smoking}
     \end{subfigure}
     % \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./3/figures/2D_Long_qualitatives/Discussionhumanposediv.eps}
         \caption{Discussion}
         \label{fig:2D_long_Discussion}
     \end{subfigure}
     \caption{Long-term prediction result by our model using 1024 linear dimension in Human3.6M dataset.}
     \label{fig:2D_long_qual}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./3/figures/good_qualitative}
    \caption{Good prediction results obtained by our model with a linear dimension of 1024.}
    \label{fig:2D_good_qual}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./3/figures/bad_qualitative}
    \caption{Bad prediction results obtained by our model with a linear dimension of 1024.}
    \label{fig:2D_bad_qual}
\end{figure}

\section{Summary}
This research is conducted to set the baseline of 2D human motion forecasting, which is applicable to most systems that use RGB cameras. In this case, one can also see it as a reliable alternative to the 3D-based data of human motion forecasting. The author proposed the time-series self-attention as a method to predict human motion for the short and long term. This study compared the time-series self-attention method with the LSTM and GRU models. 

The author evaluates the models based on the MPJPE to measure the error from the prediction to the ground truth, MPJVE to evaluate the movement of every frame in pixels, and MPJLE to calculate the average correct key points in the threshold tolerance value. This study also compared the result when the data obtained by the pose estimation method is used. In this case, the author uses OpenPose as a standard method. In addition, the average computation time of our method is calculated to see the real-time usability. 
As a result, our method could predict short-term and long-term human motion well. Based on the MPJPE, the author found out that the model predicted better on the long-term prediction task than in the short-term. Meanwhile, on the 3DPW dataset, the prediction results for the long-term prediction from our method, LSTM, and GRU models obtained a considerably significant MPJPE metric. The average computation time is below our expectation to be applicable in the real-time system. Although the average computation time relies on the computation device, this issue is still can be solved by reducing the linear dimension on the MLP Head, reducing the number of heads in the self-attention layer, reducing the number of the transformer encoder layer, and also changing the type of data input to Float16. However, this issue needs further research to do to evaluate the result.

While the author strongly believes that the outcome of the method is giving an impact as the baseline for future work in this related field of study. Thus, future relevant work could advance the result based on the evaluation metrics and the average computational time cost.

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------